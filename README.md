# polyscript ðŸ¦€

> Rust CLI that dispatches to **16 languages** via FFI bridges and subprocess.  
> 187 lines of `src` across 5 files. Two macros generate 13 language bridges.

---

## Table of contents

- [Why polyscript?](#why-polyscript)
- [Languages](#languages)
- [Install](#install)
- [Quick start](#quick-start)
- [Subcommand reference](#subcommand-reference)
- [Bridge architecture](#bridge-architecture)
- [Data IPC](#data-ipc)
- [Known limitations](#known-limitations)
- [Roadmap](#roadmap)
- [License](#license)

---

## Why polyscript?

Each language excels at a different task. Wiring them together usually means writing per-language subprocess boilerplate scattered across your codebase:

```bash
# before â€” bespoke invocation per language
python3 preprocess.py  /data/raw.parquet  /tmp/features.arrow
julia   simulate.jl    /tmp/features.arrow /tmp/result.arrow
Rscript plot.r         /tmp/result.arrow   /tmp/figures/
```

polyscript gives a single binary, single interface, and consistent exit-code propagation:

```bash
# after â€” one CLI for every language
polyscript py   preprocess.py  /data/raw.parquet   /tmp/features.arrow
polyscript jl   simulate.jl    /tmp/features.arrow  /tmp/result.arrow
polyscript r    plot.r         /tmp/result.arrow    /tmp/figures/
```

At the Rust level, 13 of those bridges are generated by a single macro line each:

```rust
sp_bridge!(julia, "julia");
sp_bridge!(go,    "go",   "run");
sp_bridge!(js,    "node");
// ... 10 more
```

---

## Languages

| Subcommand | Runtime | Bridge | Startup | Best for |
|---|---|---|---|---|
| `py` | Python | PyO3 in-process | **zero** | ML / scripting |
| `cpp` | C++ `.so` | libloading | **zero** | SIMD, native libs |
| `jl` | Julia | juliac AOT compile+run | zero (AOT) | ODE, SciML, numerics |
| `go` | Go | subprocess | ~30 ms | CLI tools, REST APIs |
| `js` | Node.js | subprocess | ~40 ms | JSON, npm ecosystem |
| `ts` | Deno | subprocess | ~60 ms | TypeScript, WASM |
| `lua` | Lua | subprocess | ~10 ms | embedded config, game logic |
| `r` | R | subprocess | ~300 ms | stats, ggplot2, CRAN |
| `mojo` | Mojo | subprocess | ~200 ms | Python superset + GPU |
| `zig` | Zig | subprocess | ~50 ms | C replacement, cross-compile |
| `wasm` | wasmtime | subprocess | ~80 ms | sandboxed plugins |
| `hs` | GHC | subprocess | ~150 ms | DSL, type-level correctness |
| `swift` | Swift | subprocess | ~200 ms | Apple SDK, Embedded Swift |
| `kt` | Kotlin | subprocess | ~1 s | JVM, Android, coroutines |
| `nim` | Nim | subprocess | ~100 ms | C-speed scripting |
| `fort` | gfortran | compile+run | compile+~5 ms | HPC, CFD, legacy solvers |
| `run` | polyscript.toml | alias dispatch | â€” | script registry |
| `parallel` | any | thread-parallel | â€” | concurrent dispatch |

---

## Install

### From source

```bash
git clone https://github.com/fumishiki/polyscript
cd polyscript
cargo build --release
# binary: ./target/release/polyscript
```

Requires **Python development headers** for PyO3. Each subcommand additionally needs its runtime binary in `$PATH`.

### Runtime prerequisites (quick reference)

| Language | Install |
|---|---|
| Python | pyenv / system package |
| Julia | [julialang.org/downloads](https://julialang.org/downloads/) |
| Go | [go.dev/dl](https://go.dev/dl/) |
| Node.js | [nodejs.org](https://nodejs.org/) |
| Deno | `curl -fsSL https://deno.land/install.sh \| sh` |
| R | `brew install r` / `apt install r-base` |
| Zig | [ziglang.org/download](https://ziglang.org/download/) |
| wasmtime | `curl https://wasmtime.dev/install.sh -sSf \| bash` |
| Kotlin | `brew install kotlin` / SDKMAN |
| Others | system package manager |

---

## Quick start

```bash
# Python â€” in-process via PyO3, zero subprocess overhead
polyscript py  scripts/python/example.py  hello world

# Julia â€” juliac AOT compile+run (requires Julia 1.12+)
polyscript jl  scripts/julia/example.jl   hello world

# Go â€” `go run` subprocess
polyscript go  scripts/go/example.go      hello world

# TypeScript via Deno
polyscript ts  scripts/ts/example.ts      hello world

# R
polyscript r   scripts/r/example.r        hello world

# C++ shared library â€” compile first, then invoke named symbol
g++ -shared -fPIC -o /tmp/libex.dylib scripts/cpp/example.cpp
polyscript cpp /tmp/libex.dylib run    hello world

# Fortran â€” compile + run in one step
polyscript fort scripts/fortran/example.f90  hello world

# WebAssembly (compile .wat â†’ .wasm first)
wat2wasm scripts/wasm/example.wat -o /tmp/example.wasm
polyscript wasm /tmp/example.wasm

# polyscript.toml alias
polyscript run preprocess /data/raw.parquet /tmp/features.arrow

# Parallel execution â€” runs py and r concurrently in separate threads
polyscript parallel "py scripts/python/example.py hello" "r scripts/r/example.r hello"

# IPC â€” auto-generate POLYSCRIPT_IPC_PATH and inject into subprocess env
polyscript --ipc-format=arrow py scripts/python/example.py
```

---

## Subcommand reference

### Shared signature (all script-based commands)

```
polyscript <COMMAND> <script> [args...]
```

`<script>` is forwarded as the first positional argument to the runtime.  
`[args...]` are forwarded verbatim to the script's argument vector.

### C++ â€” special signature

```
polyscript cpp <lib> <func> [args...]
```

The shared library must export:

```cpp
extern "C" int run(int argc, const char** argv) {
    // return 0 on success
}
```

### Args convention per language

| Subcommand | How to read args in script |
|---|---|
| `py` | `sys.argv[1:]` |
| `jl` | `ARGS` |
| `go` | `os.Args[1:]` |
| `js` | `process.argv.slice(2)` |
| `ts` | `Deno.args` |
| `lua` | `arg[1]`, `arg[2]`, â€¦ |
| `r` | `commandArgs(trailingOnly=TRUE)` |
| `zig` | `std.process.args()` |
| `hs` | `System.Environment.getArgs` |
| `swift` | `CommandLine.arguments.dropFirst()` |
| `kt` | `args` (script-scope array) |
| `nim` | `commandLineParams()` |
| `fort` | `get_command_argument(i, arg)` |

---

## Bridge architecture

```
polyscript
â”œâ”€â”€ bridge::python   PyO3 in-process          python.rs  (20 lines)
â”œâ”€â”€ bridge::cpp      libloading dynamic load   cpp.rs     (23 lines)
â””â”€â”€ bridge::mod      sp() / cr() + macros      mod.rs     (63 lines)
     â”‚
     â”œâ”€ sp(cmd, pre[], script, args[])
     â”‚    â””â”€ Command::new(cmd).args(pre).arg(script).args(args).status()
     â”‚
     â”œâ”€ cr(compiler, flags[], src, args[])
     â”‚    â”œâ”€ compile: Command::new(compiler).args(flags).arg(src).arg("-o").arg(tmp)
     â”‚    â””â”€ run:     Command::new(tmp).args(args)
     â”‚
     â”œâ”€ sp_bridge!(mod, cmd [, pre...])
     â”‚    â””â”€ pub mod $mod { pub fn run(s,a) { super::sp($cmd, &[$pre..], s, a) } }
     â”‚
     â””â”€ cr_bridge!(mod, compiler)
          â””â”€ pub mod $mod { pub fn run(s,a) { super::cr($compiler, &[], s, a) } }
```

All 13 subprocess bridges in `mod.rs`:

```rust
sp_bridge!(julia, "julia");
sp_bridge!(go,    "go",       "run");
sp_bridge!(js,    "node");
sp_bridge!(ts,    "deno",     "run");
sp_bridge!(lua,   "lua");
sp_bridge!(r,     "Rscript");
sp_bridge!(mojo,  "mojo");
sp_bridge!(zig,   "zig",      "run");
sp_bridge!(wasm,  "wasmtime", "run");
sp_bridge!(hs,    "runghc");
sp_bridge!(swift, "swift");
sp_bridge!(kt,    "kotlinc",  "-script");
sp_bridge!(nim,   "nim",      "r");
cr_bridge!(fort,  "gfortran");
```

`main.rs` uses a shared `S { script, args }` struct so each of the 15 script subcommands is a one-liner in the `match` dispatch:

```rust
#[derive(Args)]
struct S { script: String, args: Vec<String> }

match Cli::parse().cmd {
    Py(a)   => python::run(&a.script, &a.args),
    Jl(a)   => julia::run(&a.script, &a.args),
    Go(a)   => go::run(&a.script, &a.args),
    // ...
    Cpp { lib, func, args } => cpp::run(&lib, &func, &args),
}
```

---

## Data IPC

polyscript passes **strings only** across language boundaries.  
For structured data, use file paths as arguments with these conventions:

| Data size | Recommended method |
|---|---|
| Flags, IDs, counters | CLI `[args...]` |
| MB to hundreds of MB | Parquet / Arrow IPC file path as `args[1]` |
| GB+ or in-process | Python (PyO3) / C++ (libloading) â€” shared memory / buffer protocol |
| Streaming | stdin/stdout pipe with NDJSON or MessagePack |

### Arrow IPC pipeline example

```bash
polyscript py   preprocess.py  /data/raw.parquet   /tmp/features.arrow
polyscript jl   simulate.jl    /tmp/features.arrow  /tmp/result.arrow
polyscript r    plot.r         /tmp/result.arrow    /tmp/figures/
```

```python
# preprocess.py
import sys, pyarrow as pa, pyarrow.parquet as pq, pyarrow.ipc as ipc
df = pq.read_table(sys.argv[1])
with ipc.new_file(sys.argv[2], df.schema) as w:
    w.write_table(df)
```

```julia
# simulate.jl
using Arrow, DataFrames
df = DataFrame(Arrow.Table(ARGS[1]))
Arrow.write(ARGS[2], df)
```

---

## Known limitations

- **Kotlin cold-start** â€” ~1 s JVM boot.
- **Julia requires juliac 1.12+** â€” experimental AOT compiler; Julia 1.12+ required.
- **Concurrent `fort`** â€” both calls overwrite `/tmp/polyscript_out`. Mitigation: per-PID path (planned).
- **No structured IPC** â€” data contracts are the caller's responsibility. Arrow IPC file path is the recommended workaround.
- **Wasm `.wat` requires pre-compilation** â€” `wat2wasm example.wat -o example.wasm` before use.

---

## License

Licensed under either of [MIT](LICENSE-MIT) or [Apache-2.0](LICENSE-APACHE) at your option.
